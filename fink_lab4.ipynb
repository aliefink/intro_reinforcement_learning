{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Please start by making your own copy of this Google Colab notebook. You can upload your finished version [here](https://drive.google.com/drive/folders/16jdlUw6f_oOndT9IE7FlTM4Y60Vxu7cK?usp=sharing). \n",
        "\n",
        "Learning objectives:  \n",
        "\n",
        "* Implement a model-free agent\n",
        "* Implement a model-based agent\n",
        "* Compare their behavior under different transition dynamics\n",
        "\n",
        "The cell below imports some necessary modules:"
      ],
      "metadata": {
        "id": "lSVV6iLFQq2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  \n",
        "from scipy.stats import norm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RFDVLwReQrWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define environment"
      ],
      "metadata": {
        "id": "ebqWDa4vx1wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMaze(object):\n",
        "  \n",
        "  \"\"\"Class for the simple maze environment from lecture, with a slip probability. \n",
        "  This is a 3-state maze in which the agent makes a two-step decision whether \n",
        "  to go left or right. \n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  p_slip : float, (0, 1)\n",
        "      The probability that the agent \"slips\": that is, it goes right if it selects \n",
        "      the left action, or left if it selects the right action\n",
        "      \n",
        "      Note: this applies only to the starting state. \n",
        " \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, p_slip):\n",
        "\n",
        "    # initialize state space\n",
        "    # state 1 = START DOOR, state 2 = LEFT DOOR, state 3 = RIGHT DOOR\n",
        "    self.n_states = 3\n",
        "    self.state_space = list(np.arange(self.n_states)+1) + [0] # 0 is terminal state\n",
        "\n",
        "    # initialize action space\n",
        "    self.action_space = [1, 2]\n",
        "    self.k = len(self.action_space)\n",
        "   \n",
        "    # initialize parameters \n",
        "    self.p_slip = p_slip    # transition probability\n",
        "\n",
        "    # initialize timestep and state\n",
        "    self.t = 0\n",
        "    self.state = 1  \n",
        "\n",
        "    print('timestep: ' + str(self.t))\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    # re-initialize timestep\n",
        "    self.t = 0\n",
        "\n",
        "    # resets environment to initial state\n",
        "    return self.state_space[0]\n",
        "  \n",
        "  def step(self, state, action, verbose=True):\n",
        "\n",
        "    assert state in self.state_space, \"Invalid state\" \n",
        "    assert action in self.action_space, \"Invalid action\" \n",
        "\n",
        "\n",
        "    if verbose == True:\n",
        "\n",
        "      if state == 1:\n",
        "        print('at start door')\n",
        "\n",
        "      if action == 1:\n",
        "        print('going left...')\n",
        "      else:\n",
        "        print('going right...')\n",
        "\n",
        "    # the step method takes as input a state and action and changes the environment\n",
        "    # action 1 = LEFT, action 2 = RIGHT \n",
        "\n",
        "    if state == 1:         # at start door \n",
        "\n",
        "        # generate random number between 0 and 1.\n",
        "        # this draws from a uniform distribution, so we have an equal probability\n",
        "        # of generating any real number between 0 and 1.\n",
        "        p = np.random.rand()\n",
        "        \n",
        "        if action == 1:    # agent picks left \n",
        "\n",
        "          # check if agent slipped\n",
        "          if p < self.p_slip: \n",
        "\n",
        "            print('oops, slipped!')\n",
        "\n",
        "            new_state = 3   \n",
        "          else: \n",
        "            new_state = 2\n",
        "        \n",
        "        else:              # agent picks right\n",
        "          \n",
        "          # check if agent slipped\n",
        "          if p < self.p_slip: \n",
        "\n",
        "            print('oops, slipped!')\n",
        "\n",
        "            new_state = 2   \n",
        "          else: \n",
        "            new_state = 3\n",
        "        \n",
        "        reward = 0\n",
        "\n",
        "    elif state == 2:      # reached left door\n",
        "        \n",
        "        if action == 1:   # agent picks left\n",
        "         \n",
        "          new_state = 0\n",
        "          reward = 4\n",
        "        \n",
        "        else:             # agent picks right\n",
        "          \n",
        "          new_state = 0\n",
        "          reward = 0\n",
        "    \n",
        "    else:                 # reached right door\n",
        "\n",
        "        if action == 1:   # agent picks left\n",
        "          \n",
        "          new_state = 0\n",
        "          reward = 2\n",
        "        \n",
        "        else:             # agent picks right\n",
        "          \n",
        "          new_state = 0\n",
        "          reward = 1\n",
        "\n",
        "    # increment timestep\n",
        "    self.t = self.t + 1\n",
        "\n",
        "    if verbose == True:\n",
        "\n",
        "      print('reward: ' + str(reward))\n",
        "      print('new state: ' + str(new_state))\n",
        "      \n",
        "      if new_state == 2: \n",
        "        print('reached left door')\n",
        "      if new_state == 3: \n",
        "        print('reached right door')\n",
        "      if new_state == 0: \n",
        "        print('reached terminal state')\n",
        "\n",
        "      print('timestep: ' + str(self.t))\n",
        "\n",
        "    return reward, new_state\n"
      ],
      "metadata": {
        "id": "xCGV08kiRRMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test environment"
      ],
      "metadata": {
        "id": "4L7VOqYxb4Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinisic case\n",
        "p_slip = 0\n",
        "\n",
        "maze = SimpleMaze(p_slip)\n",
        "\n",
        "print(\"state space:\")\n",
        "print(maze.state_space)\n",
        "\n",
        "print(\"action space:\")\n",
        "print(maze.action_space)\n",
        "\n",
        "state = maze.reset()\n",
        "\n",
        "# Going left twice in a row\n",
        "r, new_state = maze.step(state, 1)\n",
        "r, new_state = maze.step(new_state, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPNQ9xHsb8VR",
        "outputId": "e2063562-2862-4074-f720-80a9c95eeef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestep: 0\n",
            "state space:\n",
            "[1, 2, 3, 0]\n",
            "action space:\n",
            "[1, 2]\n",
            "at start door\n",
            "going left...\n",
            "reward: 0\n",
            "new state: 2\n",
            "reached left door\n",
            "timestep: 1\n",
            "going left...\n",
            "reward: 4\n",
            "new state: 0\n",
            "reached terminal state\n",
            "timestep: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilistic case\n",
        "p_slip = 0.9\n",
        "\n",
        "maze = SimpleMaze(p_slip)\n",
        "\n",
        "print(\"state space:\")\n",
        "print(maze.state_space)\n",
        "\n",
        "print(\"action space:\")\n",
        "print(maze.action_space)\n",
        "\n",
        "state = maze.reset()\n",
        "\n",
        "# Go left twice in a row\n",
        "r, new_state = maze.step(state, 1)\n",
        "r, new_state = maze.step(new_state, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xKEgTT8iUxX",
        "outputId": "b68b52e5-db48-453a-dfe7-b1ce9b46315b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestep: 0\n",
            "state space:\n",
            "[1, 2, 3, 0]\n",
            "action space:\n",
            "[1, 2]\n",
            "at start door\n",
            "going left...\n",
            "oops, slipped!\n",
            "reward: 0\n",
            "new state: 3\n",
            "reached right door\n",
            "timestep: 1\n",
            "going left...\n",
            "reward: 2\n",
            "new state: 0\n",
            "reached terminal state\n",
            "timestep: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: model-free learning \n",
        "Let's start by defining a Q-Learning agent that learns to navigate the maze from experience with no knowledge of the model.  "
      ],
      "metadata": {
        "id": "YRXpgEQ_x9Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning(object):\n",
        "  \"\"\"Class for the Q-learning algorithm. \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "\n",
        "  alpha : float, range (0, 1)\n",
        "      Learning rate.\n",
        "\n",
        "  gamma : float, range (0, 1)\n",
        "      Discount factor.\n",
        "\n",
        "  epsilon : float, range (0, 1)\n",
        "      Epsilon probability of exploration.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, env, alpha, gamma, epsilon, model_based=False, q_init=False):\n",
        "\n",
        "    # initialize action space\n",
        "    self.action_space = env.action_space\n",
        "\n",
        "    self.a = alpha\n",
        "    self.g = gamma\n",
        "    self.eps = epsilon\n",
        "\n",
        "    self.model_based = model_based\n",
        "\n",
        "     # initialize Q-values \n",
        "    if q_init: # check if initial q-values were provided \n",
        "      self.q = np.ones((env.n_states+1, env.k))*q_init     \n",
        "    else:   \n",
        "      self.q = np.zeros((env.n_states+1, env.k))\n",
        "\n",
        "  def policy(self,state):\n",
        "\n",
        "    # in the epsilon-greedy case, the agent has an internal representation\n",
        "    # of the value of each action. its policy is to pick the action with the \n",
        "    # highest value with probability 1-epsilon and explore a random action with \n",
        "    # probabilty epsilon.  \n",
        "\n",
        "    # generate random number between 0 and 1.\n",
        "    # this draws from a uniform distribution, so we have an equal probability\n",
        "    # of generating any real number between 0 and 1.\n",
        "    p = np.random.rand()\n",
        "\n",
        "    # select action\n",
        "    if (p < self.eps): # is the number we drew is smaller than epsilon? \n",
        "      action = np.random.choice(env.k)+1 # random action\n",
        "    else: \n",
        "      # choose argmax, breaking ties randomly \n",
        "      action = np.random.choice(np.flatnonzero(self.q[state-1] == self.q[state-1].max())) + 1\n",
        "    \n",
        "    return action\n",
        "\n",
        "  def update(self, current_state, action, reward, new_state, verbose=False):\n",
        "\n",
        "    # we are at the start door \n",
        "    if current_state == 1:\n",
        "\n",
        "      if self.model_based: \n",
        "\n",
        "        # model-based learning\n",
        "        # planning to go left\n",
        "        self.q[current_state-1, 0] = (1-env.p_slip) * max(self.q[1,:]) + env.p_slip * max(self.q[2,:])\n",
        "        # planning to go right\n",
        "        self.q[current_state-1, 1] = env.p_slip * max(self.q[1,:]) + (1-env.p_slip) * max(self.q[2,:])\n",
        "      \n",
        "        \n",
        "      else: \n",
        "        # model-free learning\n",
        "        self.q[current_state-1, action-1] = self.q[current_state-1, action-1] + self.a*(reward + self.g * max(self.q[new_state-1, :]) - self.q[current_state-1, action-1])\n",
        "  \n",
        "    # we are at one of the second doors\n",
        "    elif (current_state == 2) | (current_state == 3):\n",
        "\n",
        "      # model-free learning\n",
        "      self.q[current_state-1, action-1] = self.q[current_state-1, action-1] + self.a*(reward + self.g * max(self.q[new_state-1, :]) - self.q[current_state-1, action-1])\n",
        "\n",
        "    if verbose == True: \n",
        "      print(self.q)\n",
        "  "
      ],
      "metadata": {
        "id": "kRcPqPpiaP_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1A** \n",
        "\n",
        "Write a simulation that runs the Q-Learning agent for 1000 episodes in a deterministic maze environment ($p_{slip} = 0$), with a learning rate of $0.2$, no discounting, and exploration probability of $0.2$. Plot the Q-Values as a function of episode. What policy does the agent learn at convergence (i.e. what actions will it take in every state?)? "
      ],
      "metadata": {
        "id": "b0CCO9aon-NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(env_params, agent_params, n_episodes):\n",
        "\n",
        "  \"\"\"Function for running a simulation of Q learning agent in the simple maze environment.\"\"\"\n",
        "\n",
        "  # make environment\n",
        "  env = SimpleMaze(env_params['p_slip'])\n",
        "  current_state = env.reset() #move reset to beginning not within loop\n",
        "\n",
        "\n",
        "  # initialize agent \n",
        "    agent = QLearning(env, \n",
        "                  agent_params['learning_rate'], \n",
        "                  agent_params['discount_factor'], \n",
        "                  agent_params['explore_prob'], \n",
        "                  agent_params['model_based'])\n",
        "\n",
        " # initialize output containers    \n",
        "  Q_1_left = []  # state 1, left\n",
        "  Q_1_right = [] # state 1, right\n",
        "\n",
        "  Q_2_left = []  # state 2, left\n",
        "  Q_2_right = [] # state 2, right\n",
        "\n",
        "  Q_3_left = []  # state 3, left\n",
        "  Q_3_right = [] # state 3, right\n",
        " \n",
        "\n",
        "\n",
        "  for e in np.arange(n_episodes):\n",
        "\n",
        "    #got rid of inner loop- just hard coding 3 levels\n",
        "\n",
        "    \n",
        "    #first level \n",
        "    action = agent.policy(current_state) # agent takes an action\n",
        "    reward, new_state = env.step(current_state, action, verbose=False)        # environment returns reward and new state\n",
        "    agent.update(current_state, action, reward, new_state, verbose=False)     # agent updates q-values\n",
        "    current_state = new_state # current state becomes new state\n",
        "\n",
        "\n",
        "    #second level\n",
        "    action = agent.policy(current_state) # agent takes an action\n",
        "    reward, new_state = env.step(current_state, action, verbose=False)        # environment returns reward and new state\n",
        "    agent.update(current_state, action, reward, new_state, verbose=False)     # agent updates q-values\n",
        "    \n",
        "    # Store q-values for plotting\n",
        "    Q_1_left.append(agent.q[0,0])  # state 1, left\n",
        "    Q_1_right.append(agent.q[0,1]) # state 1, right\n",
        "\n",
        "    Q_2_left.append(agent.q[1,0])  # state 2, left\n",
        "    Q_2_right.append(agent.q[1,1]) # state 2, right\n",
        "\n",
        "    Q_3_left.append(agent.q[2,0])  # state 3, left\n",
        "    Q_3_right.append(agent.q[2,1]) # state 3, right\n",
        "\n",
        "    current_state = env.reset() # current state becomes new state\n",
        "\n",
        "  return Q_1_left, Q_1_right, Q_2_left, Q_2_right, Q_3_left, Q_3_right\n",
        "  "
      ],
      "metadata": {
        "id": "kTHKutPVl9tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_params = {\n",
        "      'p_slip': 0\n",
        "  }\n",
        "\n",
        "agent_params = {\n",
        "      'learning_rate': 0.2,\n",
        "      'discount_factor': 0,\n",
        "      'explore_prob': 0.2,\n",
        "      'model_based': False\n",
        "  }\n",
        "\n",
        "n_episodes = 100\n",
        "\n",
        "Q_1_left, Q_1_right, Q_2_left, Q_2_right, Q_3_left, Q_3_right = run_simulation(env_params, agent_params, n_episodes)\n",
        "\n",
        "\n",
        "# Plot as a function of episode\n",
        "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
        "plt.plot(np.array(Q_1_left), label='state 1, left');\n",
        "plt.plot(np.array(Q_1_right), label='state 1, right');\n",
        "plt.plot(np.array(Q_2_left), label='state 2, left');\n",
        "plt.plot(np.array(Q_2_right), label='state 2, right');\n",
        "plt.plot(np.array(Q_3_left), label='state 3, left');\n",
        "plt.plot(np.array(Q_3_right), label='state 3, right');\n",
        "plt.xlabel('Episode');\n",
        "plt.ylabel('Value');\n",
        "plt.title('Model free, deterministic');\n",
        "ax.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIUnNSsSmCJ4",
        "outputId": "ef156c0b-fd58-460e-b9ec-d519d732e2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestep: 0\n",
            "Ran the maze 0 times so far\n",
            "state space:\n",
            "[1, 2, 3, 0]\n",
            "2\n",
            "action space:\n",
            "1\n",
            "going left...\n",
            "reward: 4\n",
            "new state: 0\n",
            "reached terminal state\n",
            "timestep: 1\n",
            "reward: 4\n",
            "new state: 0\n",
            "[[0. ]\n",
            " [0.8]\n",
            " [0. ]\n",
            " [0. ]]\n",
            "3\n",
            "action space:\n",
            "3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-05ca579cb928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-97-c7fd538775e1>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(params, n_episodes)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_action\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#environment responds with reward based on agent's action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reward: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-027e385cf414>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, verbose)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Invalid state\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Invalid action\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Invalid action"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value\n",
        "df_wide = pd.DataFrame(V.T)\n",
        "df_wide.loc[len(df_wide.index)-1] = 10\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(9,4))\n",
        "sns.lineplot(data=df_wide, legend=True, palette='Blues', markers=True, dashes=False)\n",
        "ax.legend(labels=list(np.arange(params['n_states'])+1))\n",
        "ax.axvline(x=params['n_states']-1, color='blue',linewidth=4)\n",
        "ax.set_xticks(np.arange(params['n_states']))\n",
        "labels = [str(i) for i in np.arange(params['n_states'])+1]\n",
        "labels[-1] = 'R'\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_xlabel('State',fontsize=15);\n",
        "ax.set_ylabel('Value',fontsize=15);\n",
        "sns.set_style('white');\n",
        "sns.despine();\n",
        "\n",
        "# TD-error\n",
        "df_wide = pd.DataFrame(D.T)\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(9,4))\n",
        "sns.lineplot(data=df_wide, legend=True, palette='Greens', markers=True, dashes=False)\n",
        "ax.legend(labels=list(np.arange(params['n_states'])+1))\n",
        "ax.axvline(x=params['n_states']-1, color='green',linewidth=4)\n",
        "ax.set_xticks(np.arange(params['n_states']))\n",
        "labels = [str(i) for i in np.arange(params['n_states'])+1]\n",
        "labels[-1] = 'R'\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_xlabel('State',fontsize=15);\n",
        "ax.set_ylabel('Temporal-Difference error',fontsize=15);\n",
        "sns.set_style('white');\n",
        "sns.despine();\n",
        "\n",
        "env.visualize()"
      ],
      "metadata": {
        "id": "dEg00IpTmHso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uz6cqKhYmHoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1B** \n",
        "\n",
        "Repeat the above, but this time under a probabilistic environment ($p_{slip} = 0.7$). Does the agent's policy change? Why or why not?"
      ],
      "metadata": {
        "id": "YjyuaaYTpUbm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xy5uyAqislgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: model-based learning\n",
        "\n"
      ],
      "metadata": {
        "id": "dmlwEgfwhPyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2A**  \n",
        "Let's now assume that the transition model (ie, the probabilities of going from *State 1* to *State 2* or *State 3* given the action) is known to the agent. Modify the update function of the agent to take into account the probability that the agent will slip in *State 1*. Hint: how can you use the slip probability and the learned values at the 2nd and 3rd door to plan and make choices at the start door?\n",
        "\n",
        "Write a simulation function that runs this new agent with the same parameters as before, and plot the Q-Values at convergence. Does the agent's policy change relative to the model-free case? Why or why not? "
      ],
      "metadata": {
        "id": "dddplCv_rRxx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFhuo5I9snBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2B**  \n",
        "Suppose the agent doesn't know in advance the probability that it will slip. How could it learn this from experience? Give below an update rule for the model of the environment. Modify the update function of the agent to include this update rule. Hint: think about the successor representation (SR). "
      ],
      "metadata": {
        "id": "ffRJS2ROra4O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoBgkCaxfBig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}